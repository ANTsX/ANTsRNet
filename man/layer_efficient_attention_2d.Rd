% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/attentionUtilities.R
\name{layer_efficient_attention_2d}
\alias{layer_efficient_attention_2d}
\title{Efficient attention layer (2-D)}
\usage{
layer_efficient_attention_2d(
  object,
  numberOfFiltersFG = 4L,
  numberOfFiltersH = 8L,
  kernelSize = 1L,
  poolSize = 2L,
  doConcatenateFinalLayers = FALSE,
  trainable = TRUE
)
}
\arguments{
\item{object}{Object to compose layer with. This is either a
\link[keras:keras_model_sequential]{keras::keras_model_sequential} to add the layer to
or another Layer which this layer will call.}

\item{numberOfFiltersFG}{number of filters for F and G layers.}

\item{numberOfFiltersH}{number of filters for H. If \code{= NA}, only
use filter \code{F} for efficiency.}

\item{kernelSize}{kernel size in convolution layer.}

\item{poolSize}{pool size in max pool layer.}

\item{doConcatenateFinalLayers}{concatenate final layer with input.
Alternatively, add.  Default = FALSE}
}
\value{
a keras layer tensor
}
\description{
Wraps the EfficientAttentionLayer2D modified from the following python implementation
}
\details{
\url{https://github.com/taki0112/Self-Attention-GAN-Tensorflow}

based on the following paper:

\url{https://arxiv.org/abs/1805.08318}
}
\examples{

\dontrun{
library( keras )
library( ANTsRNet )

inputShape <- c( 100, 100, 3 )
input <- layer_input( shape = inputShape )

numberOfFiltersFG <- 64L
outputs <- input \%>\% layer_efficient_attention_2d( numberOfFiltersFG )

model <- keras_model( inputs = input, outputs = outputs )
}

}
