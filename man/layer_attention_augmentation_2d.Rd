% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/attentionUtilities.R
\name{layer_attention_augmentation_2d}
\alias{layer_attention_augmentation_2d}
\title{Attention augmentation layer (2-D)}
\usage{
layer_attention_augmentation_2d(
  object,
  depthOfQueries,
  depthOfValues,
  numberOfHeads,
  isRelative,
  trainable = TRUE
)
}
\arguments{
\item{object}{Object to compose layer with. This is either a
\link[keras:keras_model_sequential]{keras::keras_model_sequential} to add the layer to,
or another Layer which this layer will call.}

\item{depthOfQueries}{number of filters for queries.}

\item{depthOfValues}{number of filters for values.}

\item{numberOfHeads}{number of attention heads to use. It is required
that \code{depthOfQueries/numberOfHeads > 0}.}

\item{isRelative}{whether or not to use relative encodings.}

\item{trainable}{Whether the layer weights will be updated during training.}
}
\value{
a keras layer tensor
}
\description{
Wraps the AttentionAugmentation2D layer.
}
\examples{
\dontrun{
 }
}
