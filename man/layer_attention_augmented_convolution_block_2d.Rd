% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/attentionUtilities.R
\name{layer_attention_augmented_convolution_block_2d}
\alias{layer_attention_augmented_convolution_block_2d}
\title{Creates a 2-D attention augmented convolutional block}
\usage{
layer_attention_augmented_convolution_block_2d(
  inputLayer,
  numberOfOutputFilters,
  kernelSize = c(3, 3),
  strides = c(1, 1),
  depthOfQueries = 0.2,
  depthOfValues = 0.2,
  numberOfAttentionHeads = 8,
  useRelativeEncodings = TRUE
)
}
\arguments{
\item{inputLayer}{input keras layer.}

\item{numberOfOutputFilters}{number of output filters.}

\item{kernelSize}{convolution kernel size.}

\item{strides}{convolution strides.}

\item{depthOfQueries}{Defines the number of filters for the queries or \code{k}.
Either absolute or, if \code{< 1.0}, number of \code{k} filters =
\code{depthOfQueries * numberOfOutputFilters}.}

\item{depthOfValues}{Defines the number of filters for the values or \code{v}.
Either absolute or, if \code{< 1.0}, number of \code{v} filters =
\code{depthOfValues * numberOfOutputFilters}.}

\item{numberOfAttentionHeads}{number of attention heads.  Note that
\code{as.integer(kDepth/numberOfAttentionHeads)>0} (default = 8).}

\item{useRelativeEncodings}{boolean for whether to use relative encodings
(default = TRUE).}
}
\value{
a keras tensor
}
\description{
Creates a 2-D attention augmented convolutional layer as described in the paper
}
\details{
\url{https://arxiv.org/abs/1904.09925}

with the implementation ported from the following repository

\url{https://github.com/titu1994/keras-attention-augmented-convs}
}
\author{
Tustison NJ
}
