% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/attentionUtilities.R
\name{layer_attention_3d}
\alias{layer_attention_3d}
\title{Attention layer (3-D)}
\usage{
layer_attention_3d(
  object,
  numberOfChannels,
  doGoogleBrainVersion = TRUE,
  trainable = TRUE
)
}
\arguments{
\item{object}{Object to compose layer with. This is either a
\link[keras:keras_model_sequential]{keras::keras_model_sequential} to add the layer to
or another Layer which this layer will call.}

\item{numberOfChannels}{numberOfChannels}

\item{doGoogleBrainVersion}{boolean.  Variant described at second url.}

\item{trainable}{Whether the layer weights will be updated during training.}
}
\value{
a keras layer tensor
}
\description{
Wraps the AttentionLayer3D taken from the following python implementation
}
\details{
\url{https://stackoverflow.com/questions/50819931/self-attention-gan-in-keras}
\url{https://github.com/taki0112/Self-Attention-GAN-Tensorflow}

based on the following paper:

\url{https://arxiv.org/abs/1805.08318}
}
\examples{
\dontrun{
library( keras )
library( ANTsRNet )

inputShape <- c( 100, 100, 100, 3 )
input <- layer_input( shape = inputShape )

numberOfFilters <- 64
outputs <- input \%>\% layer_conv_3d( filters = numberOfFilters, kernel_size = 2 )
outputs <- outputs \%>\% layer_attention_3d( numberOfFilters )

model <- keras_model( inputs = input, outputs = outputs )
}
}
