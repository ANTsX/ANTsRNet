% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/attentionUtilities.R
\name{layer_attention_2d}
\alias{layer_attention_2d}
\title{Attention layer (2-D)}
\usage{
layer_attention_2d(object, numberOfChannels, trainable = TRUE)
}
\arguments{
\item{object}{Object to compose layer with. This is either a
\link[keras:keras_model_sequential]{keras::keras_model_sequential} to add the layer to
or another Layer which this layer will call.}

\item{numberOfChannels}{numberOfChannels}

\item{trainable}{Whether the layer weights will be updated during training.}
}
\value{
a keras layer tensor
}
\description{
Wraps the AttentionLayer2D taken from the following python implementation
}
\details{
\url{https://stackoverflow.com/questions/50819931/self-attention-gan-in-keras}

based on the following paper:

\url{https://arxiv.org/abs/1805.08318}
}
\examples{
\dontrun{
 }
}
